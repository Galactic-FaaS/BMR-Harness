# Required imports
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
import torch
from nltk.translate.bleu_score import corpus_bleu
from nltk.translate.meteor_score import single_meteor_score
from rouge_score import rouge_scorer
import numpy as np

def load_model_and_tokenizer(model_name):
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        # Assuming the model is for sequence-to-sequence tasks like translation or summarization
        translation_pipeline = pipeline("translation", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)
        return translation_pipeline
    except Exception as e:
        print(f"Error loading model or tokenizer: {e}")
        raise SystemExit

def generate_translations(translation_pipeline, inputs, max_length=128, num_return_sequences=1):
    translations = []
    for input_text in inputs:
        try:
            # Note: Adjust parameters as needed based on model capabilities and task requirements
            result = translation_pipeline(input_text, max_length=max_length, num_return_sequences=num_return_sequences)
            translations.append(result[0]['translation_text'])
        except Exception as e:
            print(f"Error during text generation: {e}")
            translations.append("")
    return translations

def evaluate_text(candidate_texts, reference_texts):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
    scores = []

    for candidate, references in zip(candidate_texts, reference_texts):
        bleu = corpus_bleu([references], [candidate.split()])
        meteor = single_meteor_score(' '.join(references), candidate)
        rouge = scorer.score(' '.join(references), candidate)

        scores.append({
            "BLEU": bleu,
            "METEOR": meteor,
            "ROUGE-1 Fmeasure": rouge['rouge1'].fmeasure,
            "ROUGE-L Fmeasure": rouge['rougeL'].fmeasure,
        })
    
    return scores

def print_scores(evaluation_scores):
    bleu_scores = [score["BLEU"] for score in evaluation_scores]
    meteor_scores = [score["METEOR"] for score in evaluation_scores]
    rouge1_scores = [score["ROUGE-1 Fmeasure"] for score in evaluation_scores]
    rougeL_scores = [score["ROUGE-L Fmeasure"] for score in evaluation_scores]

    print(f"Average BLEU Score: {np.mean(bleu_scores)}")
    print(f"Average METEOR Score: {np.mean(meteor_scores)}")
    print(f"Average ROUGE-1 Fmeasure: {np.mean(rouge1_scores)}")
    print(f"Average ROUGE-L Fmeasure: {np.mean(rougeL_scores)}")

# Example usage
model_name = "GalacticAI/DecimaXL"
translation_pipeline = load_model_and_tokenizer(model_name)

# Adapt these example inputs to your specific task
input_texts = ["This is a sentence for translation."]
generated_translations = generate_translations(translation_pipeline, input_texts)

# You would replace these with actual reference translations relevant to your task
reference_texts = [[["Ceci est une phrase pour la traduction."]]]

evaluation_scores = evaluate_text(generated_translations, reference_texts)
print_scores(evaluation_scores)
